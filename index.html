<!DOCTYPE html>
<html>
  <head>
    <title>CDD => TDD</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Droid Serif';
        background-color: black;
        /* color: white; */
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-slide-content {
        background-color: black;
        color: white;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# ¯\_(ツ)_/¯ Works on my machine: a practical approach to TDD?
*Nathan Brenner*

---

# Starting place

- Partially written features, bloated components, not very much abstraction, angular cli (some spec files but no tests),
- "You should writes tests", "If you don't write tests from the start, you'll never get around to it", "I don't have time to write tests", "Typescript avoids the need for tests"
- Components needed refactoring (abstract classes, child components, unidirectional dataflow)
- zero confidence (proof) that the code did what it was supposed to do, and the only way to know was to know every line

---

# Types of tests:
- Unit (Jasmine/mocha)
- Functional
- Integration (Protractor)
<img src="https://automationpanda.files.wordpress.com/2017/10/the-testing-pyramid.png?w=620">

---

# TDD
- Write a test that should fail
- Write only enough code to get the test to pass
- Refactor the test
- repeat
- but if you have existing code...write tests that prove the code works as it's intended
- then refactor later => Code Driven Development...and do TDD when you refactor or write new code

---

# Get your test runner to show green
- The angular cli uses Karma and Jasmine
- If you start Karma and you get long error messages or failing tests, don't panic
- comment all the content in your spec files out
- or `['it(..', 'it...'].map(it => xit));` (mark all tests as pending)

---

# Configure testbeds

```javascript
import { async, ComponentFixture, TestBed } from '@angular/core/testing';

import { LoginFormComponent } from './login-form.component';

describe('LoginFormComponent', () => {
  let component: LoginFormComponent;
  let fixture: ComponentFixture<LoginFormComponent>;

  beforeEach(async(() => {
    TestBed.configureTestingModule({
      declarations: [ LoginFormComponent ],
      imports: [],
      providers: [],
    })
    .compileComponents();
  }));

  beforeEach(() => {
    fixture = TestBed.createComponent(LoginFormComponent);
    component = fixture.componentInstance;
    fixture.detectChanges();
  });

  it('should create', () => {
    expect(component).toBeTruthy();
  });
});

```

---

# Testbed?

- testbeds are a lot like angular modules
- switch xit one at a time in one component's spec and address the first error message, save the file so the tests run again, and repeat
- Modify the `context` in test.js so that only the tests that run are the test suite you're working on
- read the errors. Figure out what they mean and how to fix them. This is actually a lot easier to do than it was in angularjs
- often you'll need to inject components that are used in the component being tested
- If you've injected services in any of these components, comment out that spec and move on (services should only be injeted in smart components). Add some inline comment in that spec for the reason why it's not commented in (like you don't know how to fix the error message)
- if you get blocked, comment out the code, and move on

## => read the docs
- If you apply what you read, it will stick

---

# Write a single `xit` statement for every method on the class

```javascript
export class LoginFormComponent {
  bar() {
    return 'bar';
  }
  foo() {
    return 'foo';
  }
}

describe('LoginFormComponent', () => {
  // beforeEach...

  it('should create', () => {
    expect(component).toBeTruthy();
  });

  xit('#bar', () => {});

  xit('#foo', () => {});
});
```

---

# write tests for the pure functions
-  no side effects
-  no modification of the state
-  returns a value
-  given the same input, always returns the same output
-  easiest tests to write
-  Use the same type of input as you would use on your app

```javascript
foo(value) {
  return value ? 'a' : 'b';
}

it('#foo returns a string', () => {
  expect(component.foo(true)).toBe('a');
});
```

---

# custom pipes, directives, reducers, services

- these are usually pure functions, and they don't require as much configuration
- 1 service that injects http (wrapper service), and is injected into the rest of the services
- this should let you abstract a lot of the repeated code like creating headers, authorization, handling errors,
- You can write tests for these methods (like calling http.get), but this can be a black hole (as is the case with integration tests)
- 1 service per model, a method per endpoint
- Tests for these service methods can just check that the endpoint req matches the api endpoint and if there's a body, the body matches what is required for the request.

---

# write tests that have site effects

- spies: test double functions
- They track information on functions that get called

```javascript
foo() {
  this.bar();
}

it('#foo calls bar', () => {
  spyOn(component, 'bar');
  component.foo();
  expect(component.bar).toHaveBeenCalled();
});
```

---

# Don't refactor yet
- make notes on tests that are hard to write
- the goal is to just improve your test coverage so you know where features work and where they don't.
- as you find different ways a feature could be used (like a method is called with different values), write additional tests for those methods
- if typescript, add types

```javascript
foo(value: string): string {
  // This method has no real value, and is just used to make a point
  return value ? 'a' : 'b';
}

it('#foo with a truthy value returns "a"', () => {
  expect(component.foo('true')).toBe('a');
});
it('#foo with a falsy value returns "b"', () => {
  expect(component.foo('')).toBe('b');
});
```

---

# Next steps:
- e2e test coverage: Page Objects, tests that check behavior and styles in one browser...then add coverage that checks other browsers
- CI/CD integration: tests get ran on every push
- multiple deployments (development and production)
  - check new features or changes to a copy of the app that live users aren't using on development instance
  - Only push throughly vetted changes to production (keep your users happy)

---

# Conclusion
- Over the course of this process, you can keep meeting the needs of your employer by adding features and fixing bugs.

- Refactoring without code coverage will just cause you more pain

- a rising tide lifts all boats: Show your team how to do this so the platform becomes more stable

- Tests will teach you how to write better code: If a test is hard to write for a method...the method is probably bad code (or you need to learn more about testing)

- "Does foo work?" => "Yes, because I have test coverage, here it is, these are the conditons on how it works, it runs every time we promote the app to production. What changes do you want on this feature?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
